# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline  # Import Pipeline

# Generate synthetic data
np.random.seed(0)
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = np.sin(X).ravel() + np.random.randn(100) * 0.5

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Function to plot underfitting and overfitting
def plot_underfit_overfit(degrees, X_train, y_train, X_test, y_test):
    plt.figure(figsize=(14, 5))
    for i, degree in enumerate(degrees):
        polynomial_features = PolynomialFeatures(degree=degree)
        linear_regression = LinearRegression()
        pipeline = Pipeline([("polynomial_features", polynomial_features),
                             ("linear_regression", linear_regression)])
        pipeline.fit(X_train, y_train)

        # Evaluate the models using cross-validation
        train_error = mean_squared_error(y_train, pipeline.predict(X_train))
        test_error = mean_squared_error(y_test, pipeline.predict(X_test))

        # Plot the results
        plt.subplot(1, len(degrees), i + 1)
        plt.scatter(X_train, y_train, edgecolor='b', s=20, label="Training data")
        plt.scatter(X_test, y_test, edgecolor='r', s=20, label="Test data")
        X_plot = np.linspace(0, 10, 100)
        plt.plot(X_plot, pipeline.predict(X_plot[:, np.newaxis]), label="Model")
        plt.xlabel("x")
        plt.ylabel("y")
        plt.title(f"Degree {degree}\nTrain error: {train_error:.2f}\nTest error: {test_error:.2f}")
        plt.legend(loc="best")
    plt.show()

# Plotting underfitting and overfitting
degrees = [1, 4, 15]
plot_underfit_overfit(degrees, X_train, y_train, X_test, y_test)

# Function to plot RMSE vs. degree of polynomial
def plot_rmse_vs_degree(degrees, X_train, y_train, X_test, y_test):
    train_errors = []
    test_errors = []
    for degree in degrees:
        polynomial_features = PolynomialFeatures(degree=degree)
        linear_regression = LinearRegression()
        pipeline = Pipeline([("polynomial_features", polynomial_features),
                             ("linear_regression", linear_regression)])
        pipeline.fit(X_train, y_train)
        train_errors.append(mean_squared_error(y_train, pipeline.predict(X_train), squared=False))
        test_errors.append(mean_squared_error(y_test, pipeline.predict(X_test), squared=False))

    plt.figure(figsize=(10, 5))
    plt.plot(degrees, train_errors, label="Train RMSE")
    plt.plot(degrees, test_errors, label="Test RMSE")
    plt.xlabel("Degree of polynomial")
    plt.ylabel("RMSE")
    plt.title("RMSE vs. Degree of Polynomial")
    plt.legend()
    plt.show()

# Plotting RMSE vs. degree of polynomial
degrees = np.arange(1, 16)
plot_rmse_vs_degree(degrees, X_train, y_train, X_test, y_test)  # Include y_test in the function call
